{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will derive and implement formulas for Gaussian Mixture Model one of the most commonly used methods for performing soft clustering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "We will need ```numpy```, ```scikit-learn```, ```matplotlib``` libraries for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import slogdet, det, solve\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from grader import Grader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging we will use samples from gaussian mixture model with unknown mean, variance and priors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf0f8de320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = np.load('samples.npz')\n",
    "X = samples['data']\n",
    "pi0 = samples['pi0']\n",
    "mu0 = samples['mu0']\n",
    "sigma0 = samples['sigma0']\n",
    "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{P(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
    "\n",
    "<b>E-step</b>:<br>\n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
    "<b>M-step</b>:<br> \n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
    "\n",
    "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
    "\n",
    "Latent variables $T$ are indices of components to which each data point is assigned. $T_i$ (cluster index for object $i$) is a binary vector with only one active bit in position corresponding to the true component. For example, if we have $C=3$ components and object $i$ lies in first component, $T_i = [1, 0, 0]$.\n",
    "\n",
    "The joint distribution can be written as follows: $p(T, X \\mid \\theta) =  \\prod\\limits_{i=1}^N p(T_i, X_i \\mid \\theta) = \\prod\\limits_{i=1}^N \\prod\\limits_{c=1}^C [\\pi_c \\mathcal{N}(X_i \\mid \\mu_c, \\Sigma_c)]^{T_{ic}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step\n",
    "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q(T) = p(T|X, \\theta)$. We will assume that $T_i$ (cluster index for object $i$) is a binary vector with only one '1' in position corresponding to the true component. To do so we need to compute $\\gamma_{ic} = P(T_{ic} = 1 \\mid X, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$. When you compute exponents of large numbers, you get huge numerical errors (some numbers will simply become infinity). You can avoid this by dividing numerator and denominator by $e^{\\max(x)}$: $\\frac{e^{x_i-\\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. This trick is called log-sum-exp. So, to compute desired formula you first subtract maximum value from each component in vector $X$ and then compute everything else as before.\n",
    "\n",
    "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to solve the equation $Ay = x$. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by Gaussian elimination procedure. You can use ```np.linalg.solve``` for this.\n",
    "\n",
    "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 1:</b> Implement E-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    pi: (C), mixture component weights \n",
    "    mu: (C x d), mixture component means\n",
    "    sigma: (C x d x d), mixture component covariance matrices\n",
    "    \n",
    "    Returns:\n",
    "    gamma: (N x C), probabilities of clusters for objects\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # distribution q(T)\n",
    "\n",
    "    for i in range(C):\n",
    "        model = multivariate_normal(mean=mu[i,:], cov=sigma[i,:])\n",
    "        gamma[:,i] = model.pdf(X)\n",
    "    gamma *= pi\n",
    "    gamma = gamma/gamma.sum(axis=1, keepdims=True)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.89984413e-001 1.00155874e-002 2.51121517e-016]\n",
      " [9.77802155e-001 2.21978447e-002 1.07229089e-014]\n",
      " [9.78228436e-001 2.17715636e-002 6.65282252e-016]\n",
      " [9.99470994e-001 5.29005851e-004 7.87626322e-024]\n",
      " [9.99482344e-001 5.17656126e-004 3.24388347e-026]\n",
      " [6.83857110e-005 9.99931614e-001 1.15852408e-020]\n",
      " [1.50729715e-036 1.00000000e+000 2.89692453e-057]\n",
      " [1.83115480e-001 8.16884375e-001 1.44924709e-007]\n",
      " [9.60027562e-001 3.99724376e-002 7.78636239e-013]\n",
      " [4.35705967e-001 5.33717874e-001 3.05761590e-002]\n",
      " [4.22427127e-001 5.74733674e-001 2.83919872e-003]\n",
      " [9.88286857e-001 1.17131434e-002 1.83043464e-016]\n",
      " [5.42939168e-001 4.56729246e-001 3.31586125e-004]\n",
      " [1.08502493e-001 8.91497507e-001 8.73826666e-032]\n",
      " [6.27715071e-004 9.99372285e-001 3.50074230e-024]\n",
      " [8.65821552e-001 1.34178197e-001 2.50489977e-007]\n",
      " [1.34635440e-010 1.00000000e+000 1.69888046e-010]\n",
      " [9.98574647e-001 1.42535271e-003 3.60305312e-021]\n",
      " [5.82682916e-001 4.17155213e-001 1.61870957e-004]\n",
      " [7.17431881e-003 9.92825681e-001 6.50764568e-033]\n",
      " [8.84438735e-001 1.15561258e-001 7.44147679e-009]\n",
      " [3.31022532e-032 1.00000000e+000 7.36124776e-060]\n",
      " [9.02677772e-001 9.73222282e-002 5.41088147e-013]\n",
      " [5.00471682e-001 4.99528299e-001 1.83893219e-008]\n",
      " [4.27581828e-001 5.72418172e-001 6.37019789e-016]\n",
      " [9.75523255e-001 2.44767447e-002 2.74486756e-013]\n",
      " [1.51244867e-001 8.48692121e-001 6.30121146e-005]\n",
      " [9.96672991e-001 3.32700876e-003 4.95455456e-019]\n",
      " [9.98159095e-001 1.84090520e-003 7.83038379e-023]\n",
      " [5.37164837e-001 4.62835163e-001 1.90763710e-010]\n",
      " [2.73400571e-001 7.26599429e-001 1.02566952e-017]\n",
      " [7.75524307e-011 1.00000000e+000 5.24212154e-018]\n",
      " [7.22662545e-005 9.99927692e-001 4.15278765e-008]\n",
      " [6.74049906e-002 9.32578071e-001 1.69382852e-005]\n",
      " [9.46306123e-001 5.36938771e-002 2.96276141e-011]\n",
      " [1.92106543e-001 8.07893457e-001 3.42577586e-024]\n",
      " [9.83432083e-001 1.65679174e-002 2.61226097e-014]\n",
      " [9.98027352e-001 1.97264791e-003 9.02095624e-021]\n",
      " [9.67467153e-001 3.25328469e-002 1.20353992e-020]\n",
      " [7.77654827e-001 2.22331092e-001 1.40810861e-005]\n",
      " [4.25072622e-029 1.00000000e+000 2.97536994e-047]\n",
      " [9.82169125e-040 1.00000000e+000 8.12086589e-032]\n",
      " [3.39114692e-006 9.99996609e-001 6.00751693e-011]\n",
      " [9.88800549e-001 1.11994512e-002 3.80896653e-018]\n",
      " [1.16713858e-012 1.00000000e+000 3.46337116e-022]\n",
      " [3.10046573e-005 9.99703513e-001 2.65482523e-004]\n",
      " [7.46640884e-010 9.99999696e-001 3.03518301e-007]\n",
      " [9.25711181e-001 7.42888187e-002 3.08664660e-012]\n",
      " [8.80869712e-001 1.19130288e-001 1.78095586e-015]\n",
      " [8.75967464e-002 9.12403045e-001 2.08461846e-007]\n",
      " [8.18300513e-001 1.81693445e-001 6.04150639e-006]\n",
      " [1.16688886e-005 9.99988329e-001 1.70344337e-009]\n",
      " [3.38337942e-005 9.99790740e-001 1.75426480e-004]\n",
      " [5.33087028e-056 1.00000000e+000 2.52623256e-099]\n",
      " [9.89207035e-001 1.07929655e-002 5.30875614e-023]\n",
      " [3.51715656e-005 9.99963909e-001 9.19924860e-007]\n",
      " [1.87244905e-040 1.00000000e+000 1.13498824e-071]\n",
      " [7.68782909e-001 2.31217091e-001 2.99659994e-020]\n",
      " [6.67156178e-001 3.32801970e-001 4.18520525e-005]\n",
      " [2.02966620e-003 9.97970334e-001 8.51309898e-016]\n",
      " [9.79084486e-006 9.99990137e-001 7.26278391e-008]\n",
      " [9.89554543e-001 1.04454570e-002 1.14968358e-015]\n",
      " [4.72640759e-001 4.19676785e-001 1.07682456e-001]\n",
      " [6.19776879e-001 3.80221676e-001 1.44452637e-006]\n",
      " [9.99328626e-001 6.71374224e-004 8.68733122e-025]\n",
      " [9.81782652e-001 1.82173483e-002 9.80676287e-016]\n",
      " [6.75540240e-001 3.24459723e-001 3.76102474e-008]\n",
      " [2.80780070e-020 1.00000000e+000 4.35809573e-037]\n",
      " [1.45715243e-001 8.48877116e-001 5.40764091e-003]\n",
      " [2.64447684e-001 7.35552315e-001 6.43261727e-010]\n",
      " [8.47188297e-001 1.52811703e-001 9.96188775e-020]\n",
      " [5.68401680e-001 4.31598311e-001 8.89272789e-009]\n",
      " [7.46910771e-001 2.53089229e-001 3.04825467e-012]\n",
      " [1.39362596e-054 1.00000000e+000 4.11537544e-097]\n",
      " [8.15874991e-008 9.99999888e-001 3.02588585e-008]\n",
      " [1.27833463e-035 1.00000000e+000 7.56615280e-054]\n",
      " [7.62196158e-001 2.37803754e-001 8.80431494e-008]\n",
      " [9.93569821e-001 6.43017874e-003 3.56908029e-017]\n",
      " [6.09736578e-001 3.90263422e-001 2.37841786e-023]\n",
      " [7.97913723e-002 9.17905209e-001 2.30341863e-003]\n",
      " [9.60598218e-001 3.94017819e-002 1.36163515e-019]\n",
      " [9.98231799e-001 1.76820127e-003 1.35424876e-033]\n",
      " [3.35580255e-008 9.99999966e-001 1.79365814e-011]\n",
      " [9.81178030e-001 1.88219701e-002 7.51683946e-015]\n",
      " [1.08944684e-005 9.99989106e-001 1.48761790e-014]\n",
      " [2.41661138e-001 7.58317343e-001 2.15194567e-005]\n",
      " [9.67360243e-001 3.26397575e-002 1.76910887e-022]\n",
      " [5.04579648e-037 1.00000000e+000 4.58476022e-061]\n",
      " [9.39809319e-001 6.01906810e-002 1.14133421e-018]\n",
      " [8.93371534e-032 1.00000000e+000 2.79433794e-028]\n",
      " [9.80759102e-001 1.92408977e-002 2.75893295e-017]\n",
      " [1.66231399e-011 9.99999891e-001 1.08913209e-007]\n",
      " [4.60964846e-043 1.00000000e+000 7.00877776e-029]\n",
      " [5.56872330e-018 1.00000000e+000 1.46311874e-016]\n",
      " [3.57295644e-022 1.00000000e+000 6.25292498e-048]\n",
      " [1.00005166e-004 9.97717904e-001 2.18209077e-003]\n",
      " [9.30907197e-001 6.90928027e-002 6.03708753e-010]\n",
      " [2.94844967e-008 9.99999970e-001 2.74157816e-011]\n",
      " [2.50466557e-023 1.00000000e+000 4.80185461e-026]\n",
      " [2.73560249e-038 1.00000000e+000 3.40501104e-067]\n",
      " [7.50379332e-001 2.49620668e-001 1.54730995e-010]\n",
      " [9.94030897e-001 5.96910305e-003 1.10749254e-018]\n",
      " [1.23855867e-006 9.99998757e-001 4.70933336e-009]\n",
      " [2.63782304e-010 1.00000000e+000 6.93245402e-014]\n",
      " [1.82329505e-002 9.81767049e-001 4.17176704e-024]\n",
      " [9.76402569e-001 2.35974314e-002 6.27296885e-015]\n",
      " [2.54508734e-010 9.73066611e-001 2.69333891e-002]\n",
      " [1.33747393e-045 1.00000000e+000 8.65404174e-085]\n",
      " [8.18440306e-011 1.00000000e+000 3.68730910e-018]\n",
      " [5.45878005e-001 4.54120626e-001 1.36924354e-006]\n",
      " [5.97584204e-001 4.02415796e-001 6.81338046e-024]\n",
      " [5.51372032e-008 9.99999943e-001 2.20658482e-009]\n",
      " [6.20674312e-001 3.79325684e-001 3.53449111e-009]\n",
      " [1.13318527e-006 9.99998867e-001 1.53855599e-010]\n",
      " [5.39371081e-003 9.94604486e-001 1.80275628e-006]\n",
      " [9.61838346e-001 3.81616539e-002 9.17776316e-012]\n",
      " [5.26100330e-001 4.73880398e-001 1.92722603e-005]\n",
      " [9.49415985e-001 5.05840148e-002 9.30162183e-017]\n",
      " [7.15641239e-001 2.84358682e-001 7.89247915e-008]\n",
      " [5.05802224e-048 1.00000000e+000 1.71440517e-089]\n",
      " [9.90506273e-001 9.49372689e-003 3.42987647e-017]\n",
      " [9.87377482e-001 1.26225175e-002 6.01853831e-028]\n",
      " [9.97803736e-001 2.19626406e-003 3.91458740e-020]\n",
      " [1.30067051e-009 9.99999999e-001 5.59912198e-012]\n",
      " [6.31424827e-002 9.36857504e-001 1.35570533e-008]\n",
      " [2.27103615e-040 1.00000000e+000 1.07157827e-086]\n",
      " [8.12400365e-015 9.99999885e-001 1.14598616e-007]\n",
      " [8.81152615e-001 1.18847384e-001 1.15970699e-010]\n",
      " [6.67626004e-001 3.08066300e-001 2.43076958e-002]\n",
      " [1.27573235e-053 1.00000000e+000 5.66388480e-109]\n",
      " [9.49774594e-001 5.02254058e-002 1.57924789e-020]\n",
      " [1.00108033e-020 1.00000000e+000 1.85708367e-015]\n",
      " [6.99797547e-004 9.98854079e-001 4.46123248e-004]\n",
      " [9.28091362e-018 1.00000000e+000 3.20420718e-016]\n",
      " [9.91091100e-001 8.90889991e-003 2.82474339e-016]\n",
      " [6.91780098e-001 2.23347358e-001 8.48725433e-002]\n",
      " [6.47939355e-015 1.00000000e+000 6.99973736e-025]\n",
      " [6.80832718e-055 1.00000000e+000 2.44707762e-105]\n",
      " [1.95752248e-018 1.00000000e+000 1.87529119e-024]\n",
      " [9.92245805e-001 7.75419472e-003 6.04427257e-018]\n",
      " [9.79620375e-001 2.03796252e-002 1.14276149e-021]\n",
      " [1.70935860e-001 8.29064140e-001 4.93557916e-027]\n",
      " [9.97403502e-001 2.59649822e-003 1.36730836e-019]\n",
      " [2.20678408e-065 1.00000000e+000 1.62368574e-115]\n",
      " [5.04456661e-008 9.99999930e-001 1.93092318e-008]\n",
      " [9.99487718e-001 5.12281782e-004 5.99602271e-024]\n",
      " [9.94856654e-001 5.14334562e-003 1.46960319e-018]\n",
      " [7.01592409e-001 2.98407555e-001 3.62692873e-008]\n",
      " [9.62657354e-001 3.73426456e-002 5.66906169e-020]\n",
      " [9.92566532e-001 7.43346845e-003 5.18056089e-017]\n",
      " [1.06261963e-018 1.00000000e+000 1.28946184e-033]\n",
      " [1.18486037e-003 9.98815140e-001 4.01447086e-035]\n",
      " [9.94154042e-001 5.84595781e-003 1.40141692e-017]\n",
      " [9.98038788e-001 1.96121195e-003 9.36260484e-025]\n",
      " [9.83619351e-001 1.63806489e-002 2.36572265e-014]\n",
      " [2.56944775e-001 7.43055181e-001 4.45700250e-008]\n",
      " [5.78915756e-002 9.42108424e-001 5.17372550e-034]\n",
      " [1.06004350e-010 1.00000000e+000 4.35410111e-014]\n",
      " [2.72821660e-001 7.27178340e-001 8.11922274e-022]\n",
      " [9.94900371e-001 5.09962882e-003 6.81494020e-018]\n",
      " [1.22500920e-044 1.00000000e+000 1.98515560e-085]\n",
      " [5.78999868e-016 1.00000000e+000 4.96974601e-020]\n",
      " [2.19348185e-007 9.99999762e-001 1.87577342e-008]\n",
      " [4.18992288e-004 9.99579824e-001 1.18334909e-006]\n",
      " [1.31578952e-008 9.99999986e-001 5.53832236e-010]\n",
      " [3.20130052e-039 1.00000000e+000 3.73990464e-067]\n",
      " [6.68690087e-001 3.31309913e-001 3.53413047e-012]\n",
      " [2.16752789e-002 9.78324721e-001 1.54060256e-023]\n",
      " [2.88111444e-001 7.11886418e-001 2.13766312e-006]\n",
      " [5.88640555e-013 1.00000000e+000 5.67830209e-019]\n",
      " [4.95971696e-006 9.99995040e-001 1.92822385e-016]\n",
      " [2.52591722e-042 1.00000000e+000 1.37385708e-069]\n",
      " [1.38307432e-039 1.00000000e+000 3.65826595e-064]\n",
      " [9.49429284e-010 9.99999999e-001 1.01498864e-015]\n",
      " [5.87904097e-002 9.41209452e-001 1.38202762e-007]\n",
      " [1.77111858e-020 1.00000000e+000 7.79071993e-012]\n",
      " [2.09718240e-002 9.79026888e-001 1.28788100e-006]\n",
      " [1.54936859e-004 9.99790603e-001 5.44602310e-005]\n",
      " [1.23884492e-001 8.76115508e-001 5.06711624e-010]\n",
      " [2.75304806e-001 7.24254017e-001 4.41176620e-004]\n",
      " [9.95451944e-001 4.54805637e-003 6.27144615e-019]\n",
      " [2.63973751e-002 9.73602625e-001 2.95706962e-037]\n",
      " [9.30765376e-001 6.92346240e-002 2.09329176e-011]\n",
      " [2.10482070e-044 1.00000000e+000 2.49722156e-077]\n",
      " [7.07303446e-001 2.92696554e-001 2.05020462e-012]\n",
      " [9.50566473e-001 4.94335267e-002 1.48295719e-011]\n",
      " [8.26888599e-001 1.73111391e-001 9.07454805e-009]\n",
      " [9.53814916e-001 4.61850837e-002 5.39474984e-016]\n",
      " [7.46383836e-001 2.53613886e-001 2.27751528e-006]\n",
      " [3.23098926e-001 6.76901043e-001 3.13849242e-008]\n",
      " [1.11976789e-004 9.99888023e-001 2.72556610e-037]\n",
      " [2.24830870e-001 7.40669239e-001 3.44998908e-002]\n",
      " [9.95015368e-001 4.98463167e-003 9.07842153e-018]\n",
      " [2.23896204e-003 9.97761038e-001 1.87890187e-014]\n",
      " [9.27291025e-001 7.27089734e-002 1.12498166e-009]\n",
      " [9.07560653e-001 9.24393468e-002 1.75192832e-013]\n",
      " [5.16062836e-015 1.00000000e+000 1.99252635e-010]\n",
      " [2.61095695e-005 9.99973890e-001 2.97568965e-025]\n",
      " [9.81190290e-001 1.88097101e-002 1.73292254e-018]\n",
      " [9.91645308e-001 8.35469249e-003 3.42156472e-017]\n",
      " [9.39942457e-010 9.94597134e-001 5.40286460e-003]\n",
      " [3.15805215e-006 9.99996807e-001 3.53455584e-008]\n",
      " [7.09036304e-001 2.35409981e-001 5.55537151e-002]\n",
      " [9.63262619e-001 3.67373807e-002 2.63529411e-012]\n",
      " [9.80787759e-001 1.92122413e-002 6.73036906e-014]\n",
      " [2.69287163e-049 1.00000000e+000 7.54836665e-095]\n",
      " [9.48838197e-001 5.11618028e-002 6.61972000e-013]\n",
      " [1.76108464e-013 1.00000000e+000 2.54227957e-016]\n",
      " [1.05093550e-037 1.00000000e+000 2.56489516e-063]\n",
      " [9.24586732e-001 7.54132682e-002 1.17587385e-011]\n",
      " [9.84783363e-001 1.52166374e-002 1.74204906e-033]\n",
      " [6.38945043e-013 1.00000000e+000 3.43703956e-013]\n",
      " [9.96339217e-001 3.66078330e-003 8.40170186e-021]\n",
      " [3.12919736e-068 1.00000000e+000 1.08224688e-112]\n",
      " [8.01562247e-001 1.98437753e-001 6.48676933e-011]\n",
      " [7.92456543e-002 9.20754337e-001 8.32631915e-009]\n",
      " [9.87457753e-001 1.25422471e-002 4.51957527e-022]\n",
      " [4.97302099e-029 1.00000000e+000 5.20809361e-025]\n",
      " [5.55043097e-001 4.44956903e-001 1.19302194e-016]\n",
      " [9.98575399e-001 1.42460067e-003 1.25770732e-023]\n",
      " [5.85459932e-002 9.41454007e-001 4.49872427e-015]\n",
      " [3.60824136e-002 9.63917586e-001 3.32506472e-030]\n",
      " [4.50016386e-002 9.54988656e-001 9.70565232e-006]\n",
      " [9.97632870e-001 2.36713038e-003 6.99886574e-020]\n",
      " [1.29253978e-001 8.70746022e-001 5.61094014e-031]\n",
      " [4.99864997e-001 5.00134980e-001 2.26162963e-008]\n",
      " [3.69281581e-001 6.30718289e-001 1.30903831e-007]\n",
      " [1.06483218e-006 9.99998935e-001 2.64742717e-015]\n",
      " [9.90247153e-001 9.75284664e-003 1.18219076e-017]\n",
      " [5.04228915e-003 9.94956095e-001 1.61551439e-006]\n",
      " [1.07784716e-008 9.99999988e-001 8.83969934e-010]\n",
      " [9.95877172e-001 4.12282776e-003 5.11850341e-022]\n",
      " [6.52987748e-001 3.46996411e-001 1.58404910e-005]\n",
      " [2.86772513e-005 9.99971323e-001 1.91412278e-020]\n",
      " [9.87711197e-001 1.22888026e-002 2.57845100e-015]\n",
      " [9.95153465e-001 4.84653526e-003 3.38368554e-028]\n",
      " [1.90146693e-003 9.98036865e-001 6.16679471e-005]\n",
      " [6.25563814e-002 9.37443615e-001 3.24925488e-009]\n",
      " [3.85578210e-002 9.61442168e-001 1.14728573e-008]\n",
      " [1.43779289e-042 1.00000000e+000 3.49428099e-063]\n",
      " [2.65886419e-007 9.99999734e-001 6.52767983e-011]\n",
      " [5.06382455e-001 4.93617545e-001 1.98925805e-010]\n",
      " [2.10636124e-019 1.00000000e+000 1.26224222e-019]\n",
      " [8.20835193e-001 1.79163681e-001 1.12614912e-006]\n",
      " [9.89878985e-001 1.01210151e-002 9.02946987e-016]\n",
      " [9.82853883e-001 1.71461171e-002 3.49544666e-016]\n",
      " [8.89176676e-001 1.10823324e-001 3.22518629e-012]\n",
      " [1.26257449e-003 9.98733580e-001 3.84530064e-006]\n",
      " [9.70969439e-001 2.90305607e-002 1.73548254e-014]\n",
      " [8.42462951e-001 1.57537049e-001 8.46173790e-018]\n",
      " [7.21646037e-001 2.75183851e-001 3.17011138e-003]\n",
      " [9.61620340e-001 3.83796596e-002 1.13242061e-018]\n",
      " [8.75949620e-001 1.24050380e-001 6.98733248e-029]\n",
      " [3.30530361e-001 6.65055172e-001 4.41446693e-003]\n",
      " [9.94833983e-001 5.16601677e-003 2.56514697e-018]\n",
      " [1.95125131e-054 1.00000000e+000 1.39570857e-115]\n",
      " [5.84745800e-004 9.99414801e-001 4.53029148e-007]\n",
      " [9.01568808e-001 9.84311918e-002 1.41315990e-014]\n",
      " [9.35997646e-001 6.40023538e-002 1.68793595e-017]\n",
      " [7.29659701e-001 2.70340299e-001 4.87603953e-030]\n",
      " [1.49181000e-003 9.98508190e-001 3.46482148e-020]\n",
      " [6.47713040e-001 2.87509644e-001 6.47773159e-002]\n",
      " [9.82595277e-001 1.74047234e-002 2.56203615e-022]\n",
      " [9.17498214e-002 9.08250151e-001 2.79071020e-008]\n",
      " [9.88439907e-001 1.15600927e-002 5.37697692e-018]\n",
      " [9.52682245e-008 9.32243310e-001 6.77565948e-002]\n",
      " [9.69208947e-001 3.07910534e-002 1.49721581e-018]\n",
      " [5.92727259e-015 1.00000000e+000 1.28667531e-017]\n",
      " [9.39108900e-044 1.00000000e+000 7.39671726e-088]\n",
      " [9.94779765e-001 5.22023510e-003 1.25853856e-017]\n",
      " [3.24008057e-001 6.75991943e-001 2.86281770e-012]\n",
      " [9.98429001e-001 1.57099852e-003 6.51913419e-021]\n",
      " [3.40818929e-002 8.81493796e-001 8.44243113e-002]\n",
      " [8.19390994e-004 9.99180608e-001 6.01235060e-010]\n",
      " [9.72665254e-001 2.73347457e-002 5.76639284e-015]\n",
      " [1.55646086e-063 1.00000000e+000 2.28572762e-128]\n",
      " [7.09776182e-001 2.79484222e-001 1.07395961e-002]\n",
      " [9.98235131e-001 1.76486874e-003 1.10271134e-027]\n",
      " [9.53731841e-001 4.62681590e-002 3.85114068e-012]\n",
      " [5.58768327e-001 4.41229760e-001 1.91294604e-006]]\n"
     ]
    }
   ],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
    "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
    "\n",
    "<br>\n",
    "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 2:</b> Implement M-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    \n",
    "    Returns:\n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    pi = np.zeros(C)\n",
    "    mu = np.zeros((C, d))\n",
    "    sigma = np.zeros((C, d, d))\n",
    "    for k in range(C):\n",
    "        q_sum = gamma[:,k].sum()\n",
    "        mu[k,:] = (X*gamma[:,k][:, np.newaxis]).sum(axis=0)/ q_sum\n",
    "        sigma[k,:] = np.sum([gamma[i,k] * np.outer(X[i] - mu[k], X[i] - mu[k]) for i in range(N)], axis=0) / q_sum\n",
    "        pi[k] = q_sum / N\n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44702322 0.55076245 0.00221433] [[ 1.05852748  5.40759435]\n",
      " [ 2.16796132  2.89939188]\n",
      " [-1.33306197  1.41522785]] [[[ 0.70631457  1.00189734]\n",
      "  [ 1.00189734  3.09525744]]\n",
      "\n",
      " [[ 5.76353448  1.49049001]\n",
      "  [ 1.49049001  5.97710522]]\n",
      "\n",
      " [[ 0.0676037  -0.21186747]\n",
      "  [-0.21186747  3.29922798]]]\n"
     ]
    }
   ],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "print(pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
    "\n",
    "<b>Task 3:</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] (\\log \\pi_k + \\log \\mathcal{N}(x_n | \\mu_k, \\sigma_k)) - \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{E}[z_{n, k}] \\log \\mathbb{E}[z_{n, k}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_vlb(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \n",
    "    Returns value of variational lower bound\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    loss = 0\n",
    "    for k in range(C):\n",
    "        dist = multivariate_normal(mu[k], sigma[k],allow_singular=True)\n",
    "        for n in range(N):\n",
    "            loss += gamma[n,k]*(np.log(pi[k]+0.00001)+dist.logpdf(X[n,:])-np.log(gamma[n,k]+0.000001))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1213.96560953364\n"
     ]
    }
   ],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46383545 0.52768499 0.00847956] [[ 1.07499213  5.85685648]\n",
      " [ 2.22864219  2.45133965]\n",
      " [-1.22344612  0.84643644]] [[[ 0.68405518  0.17863387]\n",
      "  [ 0.17863387  1.3030211 ]]\n",
      "\n",
      " [[ 5.86816538  2.66859884]\n",
      "  [ 2.66859884  5.09531312]]\n",
      "\n",
      " [[ 0.02252806 -0.06231391]\n",
      "  [-0.06231391  0.97861765]]] -1150.505407633942\n"
     ]
    }
   ],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "print(pi, mu, sigma , loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have E step, M step and VLB, we can implement training loop. We will start at random values of $\\pi$, $\\mu$ and $\\Sigma$, train until $\\mathcal{L}$ stops changing and return the resulting points. We also know that EM algorithm sometimes stops at local optima. To avoid this we should restart algorithm multiple times from different starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L_i}}{\\mathcal{L_{i-1}}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
    "\n",
    "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to simply restart the procedure.\n",
    "\n",
    "<b>Task 4:</b> Implement training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10, verbose=False):\n",
    "    '''\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    X: (N, d), data points\n",
    "    C: int, number of clusters\n",
    "    '''\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = -9999999\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None\n",
    "\n",
    "    for _ in range(restarts):\n",
    "        try:\n",
    "            pi = np.array([1.0/C]*C,dtype=np.float32)\n",
    "            mu = np.random.rand(C, d)\n",
    "            sigma_ = np.random.rand(C, d, d)\n",
    "            sigma = np.array([np.dot(A, A.T) for A in sigma_])\n",
    "            prev_loss = None\n",
    "            for i in range(max_iter):\n",
    "                gamma = E_step(X, pi, mu, sigma)\n",
    "                pi, mu, sigma = M_step(X, gamma)\n",
    "                loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "                if not math.isnan(loss) and loss > best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_mu = mu\n",
    "                    best_pi = pi\n",
    "                    best_sigma = sigma\n",
    "                if verbose:\n",
    "                    print(\"Iteration {}, loss: {}\".format(i, loss))\n",
    "\n",
    "                if prev_loss is not None:\n",
    "                    diff = np.abs(loss - prev_loss)\n",
    "                    if diff < rtol:\n",
    "                        break\n",
    "                prev_loss = loss\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(sigma, mu, pi)\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: -1208.0560699500675\n",
      "Iteration 1, loss: -1196.0226717169194\n",
      "Iteration 2, loss: -1189.6701990117422\n",
      "Iteration 3, loss: -1184.0743637321314\n",
      "Iteration 4, loss: -1178.4943990852976\n",
      "Iteration 5, loss: -1172.5922808047565\n",
      "Iteration 6, loss: -1166.8431982509842\n",
      "Iteration 7, loss: -1162.2840641424807\n",
      "Iteration 8, loss: -1159.5780682000154\n",
      "Iteration 9, loss: -1158.3418965168237\n",
      "Iteration 10, loss: -1157.8353784074936\n",
      "Iteration 11, loss: -1157.61658546643\n",
      "Iteration 12, loss: -1157.5069449434384\n",
      "Iteration 13, loss: -1157.4403265639594\n",
      "Iteration 14, loss: -1157.3911469695524\n",
      "Iteration 15, loss: -1157.348511281875\n",
      "Iteration 16, loss: -1157.307049817674\n",
      "Iteration 17, loss: -1157.2634989374876\n",
      "Iteration 18, loss: -1157.2153939335196\n",
      "Iteration 19, loss: -1157.1607042883518\n",
      "Iteration 20, loss: -1157.098091260167\n",
      "Iteration 21, loss: -1157.0276392520675\n",
      "Iteration 22, loss: -1156.9515584013022\n",
      "Iteration 23, loss: -1156.8738791198002\n",
      "Iteration 24, loss: -1156.79878414951\n",
      "Iteration 25, loss: -1156.7290016300014\n",
      "Iteration 26, loss: -1156.66572768624\n",
      "Iteration 27, loss: -1156.6094420512916\n",
      "Iteration 28, loss: -1156.5604002859156\n",
      "Iteration 29, loss: -1156.5186711737106\n",
      "Iteration 30, loss: -1156.4840684251255\n",
      "Iteration 31, loss: -1156.4561387689348\n",
      "Iteration 32, loss: -1156.4342058365962\n",
      "Iteration 33, loss: -1156.4174423534166\n",
      "Iteration 34, loss: -1156.4049533203493\n",
      "Iteration 35, loss: -1156.3958569598246\n",
      "Iteration 36, loss: -1156.3893503214376\n",
      "Iteration 37, loss: -1156.3847503836287\n",
      "Iteration 38, loss: -1156.3815091942613\n",
      "Iteration 39, loss: -1156.3792084515462\n",
      "Iteration 40, loss: -1156.3775418777693\n",
      "Iteration 41, loss: -1156.3762930135679\n",
      "Iteration 42, loss: -1156.3753135671793\n",
      "Iteration 0, loss: -1218.0156825939068\n",
      "Iteration 1, loss: -1150.6528993581994\n",
      "Iteration 2, loss: -1141.8742199696119\n",
      "Iteration 3, loss: -1132.8124142571314\n",
      "Iteration 4, loss: -1121.9486328877822\n",
      "Iteration 5, loss: -1110.0570848941634\n",
      "Iteration 6, loss: -1100.8165115853787\n",
      "Iteration 7, loss: -1093.886060447486\n",
      "Iteration 8, loss: -1088.229556938896\n",
      "Iteration 9, loss: -1083.3624971862898\n",
      "Iteration 10, loss: -1078.7903330101706\n",
      "Iteration 11, loss: -1074.403183505294\n",
      "Iteration 12, loss: -1070.5087742439753\n",
      "Iteration 13, loss: -1067.530073530073\n",
      "Iteration 14, loss: -1065.6535915193756\n",
      "Iteration 15, loss: -1064.6658904119022\n",
      "Iteration 16, loss: -1064.1985514574187\n",
      "Iteration 17, loss: -1063.9851345084114\n",
      "Iteration 18, loss: -1063.8877296955372\n",
      "Iteration 19, loss: -1063.842862102717\n",
      "Iteration 20, loss: -1063.8219933400742\n",
      "Iteration 21, loss: -1063.812209779489\n",
      "Iteration 22, loss: -1063.8075957255594\n",
      "Iteration 23, loss: -1063.805410257878\n",
      "Iteration 24, loss: -1063.8043719187588\n",
      "Iteration 25, loss: -1063.8038775319048\n",
      "Iteration 0, loss: -1168.7198165067543\n",
      "Iteration 1, loss: -1147.9774185600072\n",
      "Singular matrix: components collapsed\n",
      "Iteration 0, loss: -1173.8307941563157\n",
      "Iteration 1, loss: -1140.0490087922312\n",
      "Iteration 2, loss: -1136.7090419045626\n",
      "Iteration 3, loss: -1134.6572042863022\n",
      "Iteration 4, loss: -1133.1002159591922\n",
      "Iteration 5, loss: -1131.7454967489034\n",
      "Iteration 6, loss: -1130.6948748233215\n",
      "Iteration 7, loss: -1130.1419998657434\n",
      "Iteration 8, loss: -1129.867664785802\n",
      "Iteration 9, loss: -1129.673404505972\n",
      "Iteration 10, loss: -1129.4940265937712\n",
      "Iteration 11, loss: -1129.3034144794747\n",
      "Iteration 12, loss: -1129.078677374626\n",
      "Iteration 13, loss: -1128.7875018314362\n",
      "Iteration 14, loss: -1128.3767599541216\n",
      "Iteration 15, loss: -1127.7616996627034\n",
      "Iteration 16, loss: -1126.848272225544\n",
      "Iteration 17, loss: -1125.691688376741\n",
      "Iteration 18, loss: -1124.7077130174307\n",
      "Iteration 19, loss: -1124.2157869232597\n",
      "Iteration 20, loss: -1124.040474536423\n",
      "Iteration 21, loss: -1123.999644798968\n",
      "Iteration 22, loss: -1123.9942712751606\n",
      "Iteration 23, loss: -1123.9937384809757\n",
      "Iteration 0, loss: -1255.72549313547\n",
      "Iteration 1, loss: -1190.2196667490743\n",
      "Iteration 2, loss: -1165.6514607617298\n",
      "Iteration 3, loss: -1127.4006985275907\n",
      "Iteration 4, loss: -1108.6328082632747\n",
      "Iteration 5, loss: -1103.5508066141197\n",
      "Iteration 6, loss: -1100.3115577093731\n",
      "Iteration 7, loss: -1097.2518837076175\n",
      "Iteration 8, loss: -1093.91527412659\n",
      "Iteration 9, loss: -1090.33889047796\n",
      "Iteration 10, loss: -1087.04251138201\n",
      "Iteration 11, loss: -1084.3294953489856\n",
      "Iteration 12, loss: -1081.942045366013\n",
      "Iteration 13, loss: -1079.5963973971675\n",
      "Iteration 14, loss: -1077.146752359776\n",
      "Iteration 15, loss: -1074.520499279871\n",
      "Iteration 16, loss: -1071.7561266435703\n",
      "Iteration 17, loss: -1069.1166544007135\n",
      "Iteration 18, loss: -1067.001536435424\n",
      "Iteration 19, loss: -1065.585890023026\n",
      "Iteration 20, loss: -1064.7516782287491\n",
      "Iteration 21, loss: -1064.2948218347785\n",
      "Iteration 22, loss: -1064.0534882342308\n",
      "Iteration 23, loss: -1063.928735014838\n",
      "Iteration 24, loss: -1063.8654215090887\n",
      "Iteration 25, loss: -1063.8337973592302\n",
      "Iteration 26, loss: -1063.8181989909378\n",
      "Iteration 27, loss: -1063.810575922345\n",
      "Iteration 28, loss: -1063.806874622035\n",
      "Iteration 29, loss: -1063.8050855558322\n",
      "Iteration 30, loss: -1063.8042234459556\n",
      "Iteration 0, loss: -1165.4838120186278\n",
      "Iteration 1, loss: -1142.0245471779035\n",
      "Iteration 2, loss: -1139.4917267367691\n",
      "Iteration 3, loss: -1138.7449373076058\n",
      "Iteration 4, loss: -1138.298746082023\n",
      "Iteration 5, loss: -1137.8548662966734\n",
      "Iteration 6, loss: -1137.3770556123334\n",
      "Iteration 7, loss: -1136.9014611283578\n",
      "Iteration 8, loss: -1136.4728018260596\n",
      "Iteration 9, loss: -1136.1165704605676\n",
      "Iteration 10, loss: -1135.8360155168548\n",
      "Iteration 11, loss: -1135.6206566905223\n",
      "Iteration 12, loss: -1135.455659859836\n",
      "Iteration 13, loss: -1135.3275774367035\n",
      "Iteration 14, loss: -1135.2262622835992\n",
      "Iteration 15, loss: -1135.144632080857\n",
      "Iteration 16, loss: -1135.0778036692332\n",
      "Iteration 17, loss: -1135.0223163172236\n",
      "Iteration 18, loss: -1134.9756095210428\n",
      "Iteration 19, loss: -1134.9357106059388\n",
      "Iteration 20, loss: -1134.9010541091336\n",
      "Iteration 21, loss: -1134.8703748409696\n",
      "Iteration 22, loss: -1134.842640298495\n",
      "Iteration 23, loss: -1134.817004236439\n",
      "Iteration 24, loss: -1134.792772251215\n",
      "Iteration 25, loss: -1134.7693748244799\n",
      "Iteration 26, loss: -1134.746345458409\n",
      "Iteration 27, loss: -1134.7233025434032\n",
      "Iteration 28, loss: -1134.699934075375\n",
      "Iteration 29, loss: -1134.6759845906233\n",
      "Iteration 30, loss: -1134.6512438429784\n",
      "Iteration 31, loss: -1134.625536858333\n",
      "Iteration 32, loss: -1134.5987150763951\n",
      "Iteration 33, loss: -1134.5706483300328\n",
      "Iteration 34, loss: -1134.5412174267033\n",
      "Iteration 35, loss: -1134.5103070927032\n",
      "Iteration 36, loss: -1134.4777990145024\n",
      "Iteration 37, loss: -1134.4435646529382\n",
      "Iteration 38, loss: -1134.4074574140416\n",
      "Iteration 39, loss: -1134.3693036365376\n",
      "Iteration 40, loss: -1134.3288916778884\n",
      "Iteration 41, loss: -1134.2859580882218\n",
      "Iteration 42, loss: -1134.2401693605439\n",
      "Iteration 43, loss: -1134.1910968843479\n",
      "Iteration 44, loss: -1134.1381812284649\n",
      "Iteration 45, loss: -1134.0806791984865\n",
      "Iteration 46, loss: -1134.0175821738733\n",
      "Iteration 47, loss: -1133.9474847632655\n",
      "Iteration 48, loss: -1133.8683638090422\n",
      "Iteration 49, loss: -1133.7771875604822\n",
      "Iteration 50, loss: -1133.6691846519175\n",
      "Iteration 51, loss: -1133.5363864005087\n",
      "Iteration 52, loss: -1133.364498308994\n",
      "Iteration 53, loss: -1133.1255988112632\n",
      "Iteration 54, loss: -1132.7594701734195\n",
      "Iteration 55, loss: -1132.1217503476742\n",
      "Iteration 56, loss: -1130.8396545905948\n",
      "Iteration 57, loss: -1128.0658302723216\n",
      "Iteration 58, loss: -1123.2543951432333\n",
      "Iteration 59, loss: -1118.3517343196434\n",
      "Iteration 60, loss: -1115.2842696313803\n",
      "Iteration 61, loss: -1113.6908627729817\n",
      "Iteration 62, loss: -1112.6295683317317\n",
      "Iteration 63, loss: -1111.49494211611\n",
      "Iteration 64, loss: -1109.750454764456\n",
      "Iteration 65, loss: -1106.6499765091296\n",
      "Iteration 66, loss: -1101.0327420881288\n",
      "Iteration 67, loss: -1091.932040958487\n",
      "Iteration 68, loss: -1081.4200213574586\n",
      "Iteration 69, loss: -1073.314726592183\n",
      "Iteration 70, loss: -1069.073454262808\n",
      "Iteration 71, loss: -1066.5395596062099\n",
      "Iteration 72, loss: -1065.1101330434806\n",
      "Iteration 73, loss: -1064.4043882993487\n",
      "Iteration 74, loss: -1064.0786877013188\n",
      "Iteration 75, loss: -1063.930470424935\n",
      "Iteration 76, loss: -1063.862592658836\n",
      "Iteration 77, loss: -1063.8311891196518\n",
      "Iteration 78, loss: -1063.8165279230075\n",
      "Iteration 79, loss: -1063.8096347264866\n",
      "Iteration 80, loss: -1063.8063769110418\n",
      "Iteration 81, loss: -1063.804831483194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss: -1063.8040964451488\n",
      "Iteration 0, loss: -1210.1692970036997\n",
      "Iteration 1, loss: -1156.5651483797517\n",
      "Singular matrix: components collapsed\n",
      "Iteration 0, loss: -1239.7707017046228\n",
      "Iteration 1, loss: -1231.237543138721\n",
      "Iteration 2, loss: -1226.3070292698112\n",
      "Iteration 3, loss: -1218.8317923433717\n",
      "Iteration 4, loss: -1211.068951138456\n",
      "Iteration 5, loss: -1204.3904989015837\n",
      "Iteration 6, loss: -1198.1022642243909\n",
      "Iteration 7, loss: -1190.7392190874791\n",
      "Iteration 8, loss: -1182.5838473443746\n",
      "Iteration 9, loss: -1169.5480883481125\n",
      "Iteration 10, loss: -1171.3951750360452\n",
      "Singular matrix: components collapsed\n",
      "Iteration 0, loss: -1254.4510059548759\n",
      "Iteration 1, loss: -1165.1224600138733\n",
      "Iteration 2, loss: -1149.9593343141487\n",
      "Iteration 3, loss: -1144.7666773722765\n",
      "Iteration 4, loss: -1140.0046297014546\n",
      "Iteration 5, loss: -1135.128082336509\n",
      "Iteration 6, loss: -1131.344516078198\n",
      "Iteration 7, loss: -1129.0126743459616\n",
      "Iteration 8, loss: -1127.1795208722453\n",
      "Iteration 9, loss: -1126.0885030244099\n",
      "Iteration 10, loss: -1125.5646998788188\n",
      "Iteration 11, loss: -1125.2161607525916\n",
      "Iteration 12, loss: -1124.9120072144763\n",
      "Iteration 13, loss: -1124.5688907296833\n",
      "Iteration 14, loss: -1124.0994605574674\n",
      "Iteration 15, loss: -1123.575616129325\n",
      "Iteration 16, loss: -1123.171244057043\n",
      "Iteration 17, loss: -1122.824816795102\n",
      "Iteration 18, loss: -1122.469615984749\n",
      "Iteration 19, loss: -1122.0762113162327\n",
      "Iteration 20, loss: -1121.6369701277688\n",
      "Iteration 21, loss: -1121.1935808620315\n",
      "Iteration 22, loss: -1120.8352512722156\n",
      "Iteration 23, loss: -1120.616791689526\n",
      "Iteration 24, loss: -1120.5081022366412\n",
      "Iteration 25, loss: -1120.455261208005\n",
      "Iteration 26, loss: -1120.4266620887222\n",
      "Iteration 27, loss: -1120.4087725004836\n",
      "Iteration 28, loss: -1120.395728500147\n",
      "Iteration 29, loss: -1120.3845046141494\n",
      "Iteration 30, loss: -1120.3729487851563\n",
      "Iteration 31, loss: -1120.3586357788765\n",
      "Iteration 32, loss: -1120.3374908433373\n",
      "Iteration 33, loss: -1120.3012042592115\n",
      "Iteration 34, loss: -1120.2335003328144\n",
      "Iteration 35, loss: -1120.1177436167325\n",
      "Iteration 36, loss: -1119.9839679241954\n",
      "Iteration 37, loss: -1119.88434755029\n",
      "Iteration 38, loss: -1119.8079292894975\n",
      "Iteration 39, loss: -1119.7429859596048\n",
      "Iteration 40, loss: -1119.693011529388\n",
      "Iteration 41, loss: -1119.6621307746448\n",
      "Iteration 42, loss: -1119.6462334222526\n",
      "Iteration 43, loss: -1119.6381840938682\n",
      "Iteration 44, loss: -1119.633654687279\n",
      "Iteration 45, loss: -1119.630806695555\n",
      "Iteration 46, loss: -1119.6288723136718\n",
      "Iteration 47, loss: -1119.6274869096833\n",
      "Iteration 48, loss: -1119.6264531852742\n",
      "Iteration 49, loss: -1119.6256546180807\n",
      "Iteration 0, loss: -1156.6497987794937\n",
      "Iteration 1, loss: -1136.3868019895763\n",
      "Iteration 2, loss: -1133.3576633463829\n",
      "Iteration 3, loss: -1131.3456300258724\n",
      "Iteration 4, loss: -1130.0067869095562\n",
      "Iteration 5, loss: -1129.015259254477\n",
      "Iteration 6, loss: -1127.845445559156\n",
      "Iteration 7, loss: -1125.8694363694995\n",
      "Iteration 8, loss: -1123.1558097229913\n",
      "Iteration 9, loss: -1121.1169832346666\n",
      "Iteration 10, loss: -1119.8568939194652\n",
      "Iteration 11, loss: -1118.8927248710143\n",
      "Iteration 12, loss: -1118.0626862604272\n",
      "Iteration 13, loss: -1117.3962772236468\n",
      "Iteration 14, loss: -1116.933915820339\n",
      "Iteration 15, loss: -1116.6311657684817\n",
      "Iteration 16, loss: -1116.41338736429\n",
      "Iteration 17, loss: -1116.2438133507374\n",
      "Iteration 18, loss: -1116.1173087529571\n",
      "Iteration 19, loss: -1116.0327792218113\n",
      "Iteration 20, loss: -1115.9775718837723\n",
      "Iteration 21, loss: -1115.9350739347117\n",
      "Iteration 22, loss: -1115.8949854721407\n",
      "Iteration 23, loss: -1115.852933069276\n",
      "Iteration 24, loss: -1115.8071034213087\n",
      "Iteration 25, loss: -1115.7565859972706\n",
      "Iteration 26, loss: -1115.7009875832764\n",
      "Iteration 27, loss: -1115.6404864948886\n",
      "Iteration 28, loss: -1115.5759300958584\n",
      "Iteration 29, loss: -1115.5087458507662\n",
      "Iteration 30, loss: -1115.4405288643372\n",
      "Iteration 31, loss: -1115.3723607944614\n",
      "Iteration 32, loss: -1115.3041168251702\n",
      "Iteration 33, loss: -1115.2339677440339\n",
      "Iteration 34, loss: -1115.1579260086837\n",
      "Iteration 35, loss: -1115.068839251466\n",
      "Iteration 36, loss: -1114.9536968387438\n",
      "Iteration 37, loss: -1114.786900439004\n",
      "Iteration 38, loss: -1114.5145801008155\n",
      "Iteration 39, loss: -1114.0240372116793\n",
      "Iteration 40, loss: -1113.105889012268\n",
      "Iteration 41, loss: -1111.5019369928737\n",
      "Iteration 42, loss: -1109.5192249477961\n",
      "Iteration 43, loss: -1107.5559580698593\n",
      "Iteration 44, loss: -1105.166174021858\n",
      "Iteration 45, loss: -1102.5205327588671\n",
      "Iteration 46, loss: -1099.4039150822503\n",
      "Iteration 47, loss: -1095.4742876049763\n",
      "Iteration 48, loss: -1091.0515755495558\n",
      "Iteration 49, loss: -1086.2692683554144\n",
      "Iteration 50, loss: -1081.1615985410842\n",
      "Iteration 51, loss: -1076.161828992521\n",
      "Iteration 52, loss: -1071.796538149632\n",
      "Iteration 53, loss: -1068.4010380490274\n",
      "Iteration 54, loss: -1066.1436725162941\n",
      "Iteration 55, loss: -1064.9001305734719\n",
      "Iteration 56, loss: -1064.3030122955136\n",
      "Iteration 57, loss: -1064.0314835628833\n",
      "Iteration 58, loss: -1063.908614262167\n",
      "Iteration 59, loss: -1063.8524298784646\n",
      "Iteration 60, loss: -1063.8264344305371\n",
      "Iteration 61, loss: -1063.8142908612363\n",
      "Iteration 62, loss: -1063.8085774022977\n",
      "Iteration 63, loss: -1063.8058754428307\n",
      "Iteration 64, loss: -1063.8045930383335\n",
      "Iteration 65, loss: -1063.8039828577364\n",
      "-1063.8038775319048\n"
     ]
    }
   ],
   "source": [
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3, verbose=True)\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using matrix $\\gamma$ computed on last E-step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
